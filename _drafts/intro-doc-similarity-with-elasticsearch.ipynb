{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Document Similarity with Elasticsearch\n",
    "\n",
    "\n",
    "## Document Distance\n",
    "Short intro to document similarity - finding a way to represent the distance between documents. \n",
    "\n",
    "Two things needed:\n",
    " - The first is to encode the documents as vectors. There are options for this (one-hot, frequency, distributed). \n",
    " - The second is to decide how to measure distance - Euclidean? Manhattan? Cosine? There are tradeoffs but cosine is a good place to start with text. Why? Documents encoded as vectors are sparse; each vector could be as long as the number of unique words across the full corpus.  That means that two documents of very different lengths (e.g. a single recipe and a cookbook), could be encoded with the same length vector, which might overemphasize the magnitude of the book's document vector at the expense of the recipe's document vector. Cosine distance helps to correct for variations in vector magnitudes resulting from uneven length documents, and enables us to measure the distance between the book and recipe.\n",
    " \n",
    "Document similarity is something we cover in the book - in chapter 6 and then in chapter 10 where we build a chatbot that, among other things, uses a nearest neigbor search to recommend recipes that are similar to the ingredients listed by the user. One of my observations during the prototyping phase for that chapter is how slow vanilla nearest neighbor search is. This led me to think about different ways to optimize the search, from using variations like k-ball, to using other Python libraries like Spotify's Annoy, and also to other kind of tools altogether that attempt to deliver a similar results as quickly as possible. Like ElasticSearch!\n",
    " \n",
    "## What is Elasticsearch\n",
    "Elasticsearch is a text search engine that leverages the information retrieval library Lucene together with a key-value store to ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"/Users/rebeccabilbro/Desktop/waves/stuff/atap/code/mini_food_corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import nltk\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "from readability.readability import Document as Paper\n",
    "from readability.readability import Unparseable\n",
    "\n",
    "DOC_PATTERN = r'(?!\\.)[\\w\\s\\d\\-]+\\.html'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "# Tags to extract as paragraphs from the HTML text\n",
    "TAGS = [\n",
    "    'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li'\n",
    "]\n",
    "\n",
    "\n",
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for raw HTML documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=DOC_PATTERN,\n",
    "                 encoding='latin-1', **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "\n",
    "        self._tags = TAGS\n",
    "\n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. Implemented similarly to\n",
    "        the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of an HTML document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "\n",
    "    def titles(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses BeautifulSoup to identify titles from the\n",
    "        head tags within the HTML\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            soup = bs4.BeautifulSoup(doc, 'lxml')\n",
    "            try:\n",
    "                yield soup.title.text\n",
    "                soup.decompose()\n",
    "            except AttributeError as e:\n",
    "                continue\n",
    "                \n",
    "    def html(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the HTML content of each document, cleaning it using\n",
    "        the readability-lxml library.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            try:\n",
    "                yield Paper(doc).summary()\n",
    "            except Unparseable as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the corpus reader to read in the pre-processed documents from disk\n",
    "corpus = HTMLCorpusReader(corpus_path)\n",
    "\n",
    "# use the corpus reader's text method to retrieve the titles and descriptions of each recipe\n",
    "titles = list(corpus.titles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kid-Friendly Mac & Cheese Recipe\n"
     ]
    }
   ],
   "source": [
    "# print out the title and part-of-speech tagged description of the 501st recipe\n",
    "print(titles[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Elasticsearch\n",
    "In the command line, start elasticsearch.\n",
    "\n",
    "    $  cd elasticsearch-<version>\n",
    "    $  ./bin/elasticsearch\n",
    "\n",
    "\n",
    "## Create an Index\n",
    "Now we will create an index. Think of an index as a database in PostgreSQL or MongoDB. An Elasticsearch cluster can contain multiple _indices_ (e.g. relational or noSql databases), which in turn contain multiple _types_ (similar to MongoDB collections or PostgreSQL tables). These types hold multiple _documents_ (similart o MongoDB documents or PostgreSQL rows), and each document has _properties_ (like MongoDB document key-values or PostgreSQL columns).\n",
    "\n",
    "    $  curl -X PUT \"localhost:9200/recipes\" -H 'Content-Type: application/json' -d'\n",
    "        {\n",
    "           \"settings\" : {\n",
    "              \"number_of_shards\" : 1,\n",
    "              \"number_of_replicas\" : 1\n",
    "           }\n",
    "        }\n",
    "        '\n",
    " And the response:\n",
    " \n",
    "    {\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"recipes\"}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## Index the Recipes\n",
    "\n",
    "Now let's begin by indexing our recipe for Chocolate Peanut Butter Pie.\n",
    "\n",
    "\n",
    "    $  curl -X PUT \"localhost:9200/website/recipes\" -H 'Content-Type: application/json' -d'\n",
    "        {\n",
    "          \"title\": \"Chocolate Peanut Butter Pie\",\n",
    "          \"description\":  \"test test test\"\n",
    "        }\n",
    "        '\n",
    "\n",
    "## Search for Recipes\n",
    "\n",
    "    $  curl -XGET 'localhost:9200/website/recipes/_search?q=title:Pie&pretty'\n",
    "    \n",
    "And the response:\n",
    "\n",
    "    {\n",
    "      \"took\" : 72,\n",
    "      \"timed_out\" : false,\n",
    "      \"_shards\" : {\n",
    "        \"total\" : 5,\n",
    "        \"successful\" : 5,\n",
    "        \"skipped\" : 0,\n",
    "        \"failed\" : 0\n",
    "      },\n",
    "      \"hits\" : {\n",
    "        \"total\" : 1,\n",
    "        \"max_score\" : 0.2876821,\n",
    "        \"hits\" : [\n",
    "          {\n",
    "            \"_index\" : \"website\",\n",
    "            \"_type\" : \"recipes\",\n",
    "            \"_id\" : \"DfDRFGQBpu4qb7-sCXfn\",\n",
    "            \"_score\" : 0.2876821,\n",
    "            \"_source\" : {\n",
    "              \"title\" : \"Chocolate Peanut Butter Pie\",\n",
    "              \"description\" : \"test test test\"\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    \n",
    "Now let's do it in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_documents(index_name, index_type, texts):\n",
    "    return {\n",
    "        \"_index\": index_name,\n",
    "        \"_type\": index_type,\n",
    "        \"title\": texts[\"title\"],\n",
    "        \"description\": texts[\"document\"]\n",
    "    }\n",
    "\n",
    "def index(es_instance, index_name, index_type, texts):\n",
    "    bulk(es_instance, make_documents())\n",
    "\n",
    "def clear_es_indices(es_instance, index_name):\n",
    "    es_instance.indices.delete(\n",
    "        index=index_name,\n",
    "        ignore=[400, 404]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
